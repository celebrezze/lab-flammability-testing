---
title: "Linear Mixed Effects Models"
author: "Joe Celebrezze"
date: "2023-06-28"
output: html_document
---

# Questions
- How to deal with plant traits variables which are only measured per plant_id?
- REML = FALSE? Based on what I know, REML should = TRUE if you're mostly concerned with variance and covariance, so maybe it makes sense to keep it like this when evaluating VIF scores, but when doing AIC-based model selection, it might make more sense to do REML = FALSE (this is how I have it now, based on recent changes)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# packages
library(tidyverse)
library(here)
library(lubridate)
library(ggfortify)
library(ggpubr)
library(sjPlot) # for tab_model
library(psych) #for pairs.panels()
library(GGally) # For ggcorr()
library(scales)
library(lme4) # for lmer
library(lmerTest) # to try stepwise selection
library(remef) # for remef
library(performance) # for multicollinearity check
library(car) # for qqPlot
library(gtools) # for combination()

# defining functions
here = here::here
ggbiplot = ggbiplot::ggbiplot

# reading in data
flamm.df <- read.csv(here('data', 'processed-data', 'main_dataset.csv')) %>% 
  filter(species != 'IRIDOU')
```

# Data Structure

For the linear mixed effects models, we are going to be using the *flammability metrics* below as **dependent variables**:

- Flame height (fh)
- Flame duration (fd)
- Change in temp. (temp_change)
- Change in heat flux (heat_flux_change)

The following variables are not going to be considered (with the reasoning behind omitting each one described below):

- Time to ignition (tti): because this variable is conceptually meaningless for all manual ignitions (the majority of ignitions)
- Post-flame glow (pfg): because we do not necessarily trust the glow end time, as it was challenging to see in video playbacks when the sample stopped glowing. Because of this drawback, we relied on the people present during the flammability lab testing to clap at the end of the glow time, but the timing of this clap did not always correspond with the exact end of glowing
- Proportion ignited (prop_ig): because this variable has only one value for each species which does not fit the structure of the models

We have a plethora of variables that could be used as predictors and also a lot of potential covariates. The potential **predictors** are grouped and listed below:

*Species*: This will likely be included in every model, as understanding interspecific differences and grouping species by flammability is our central question

*Hydration or dryness*:
- LFM (can be total LFM, leaf LFM and/or stem LFM)
- Water potential (mpa)
- Wet weight (ww_flam_sample)
- Dry weight (dw_flam_sample)

*Leaf morphology*:
- LMA or SLA
- Leaf area (likely unnecessary, as described by LMA and SLA in some capacity)
- Leaf thickness

*Sample morphology*:
- Branching
- Stem to leaf ratio
- Sample weight* (note: this could also be considered a covariate)

Potential **covariates**:

- Starting temperature (start_temp)
- Sample weight* (see note above)
- Sample volume (branch_volume): could also split by height, width and length (although length is mostly the same across samples)
- Ambient humidity or temperature (note: I don't necessarily trust that the meter was doing a good job gathering this data, as it seems like the temperature of the chamber affected the measurements)
- Thermocoupler and hot plate height: probably not necessary, as exploratory analyses showed that there weren't any major differences between variables depending on the thermocoupler height or hot plate height

**Random effect**:

- Plant (to account for repeated measurements): note that we will need to make a unique identifier for each plant by combining current columns 'species' and 'plant'. Otherwise, the model will assume that (for example) plant 1 is the same group, treating HETARB 1 and SALLEU 1 as similar when they should not be treated as similar

Looking at the above variables to see how they're situated in the dataset
```{r}
# Grouping by plant_id; to check to see which traits do not have any variation per plant_id
trait.summary.plant_id <- flamm.df %>%  
  unite('plant_id', c(species, plant), sep = '_', remove = F) %>% 
  select(fh, species, lfm, leaf_lfm, stem_lfm, mpa, ww_flam_sample, dw_flam_sample, LMA, SLA, thickness, leaf_area, branching, stem_mass_ratio, leaf_mass_ratio, sample_wt, start_temp, branch_volume, ambient_humidity, ambient_temp, thermocoupler_height, hotplate_height, plant_id) %>% 
  mutate(lfm = scale(lfm), mpa = scale(mpa), ww_flam_sample = scale(ww_flam_sample),
         dw_flam_sample = scale(dw_flam_sample), LMA = scale(LMA), thickness = scale(thickness),
         leaf_area = scale(leaf_area), branching = scale(branching),
         leaf_mass_ratio = scale(leaf_mass_ratio), sample_wt = scale(sample_wt),
         start_temp = scale(start_temp), branch_volume = scale(branch_volume)) %>%  # scaling traits we want to look at so that standard deviations are comparable
  group_by(plant_id) %>% 
  dplyr::summarise(sd_lfm = sd(lfm), sd_mpa = sd(mpa), sd_ww = sd(ww_flam_sample), sd_dw = sd(dw_flam_sample),
                   sd_lma = sd(LMA), sd_thickness = sd(thickness), sd_leafarea = sd(leaf_area),
                   sd_branching = sd(branching), sd_leafratio = sd(leaf_mass_ratio), sd_sw = sd(sample_wt),
                   sd_st = sd(start_temp), sd_volume = sd(branch_volume))
trait.summary.plant_id %>% 
  mutate(sd_lfm = ifelse(is.na(sd_lfm), 0, sd_lfm),
         sd_mpa = ifelse(is.na(sd_mpa), 0, sd_mpa),
         sd_ww = ifelse(is.na(sd_ww), 0, sd_ww),
         sd_dw = ifelse(is.na(sd_dw), 0, sd_dw),
         sd_lma = ifelse(is.na(sd_lma), 0, sd_lma),
         sd_thickness = ifelse(is.na(sd_thickness), 0, sd_thickness),
         sd_leafarea = ifelse(is.na(sd_leafarea), 0, sd_leafarea),
         sd_branching = ifelse(is.na(sd_branching), 0, sd_branching),
         sd_leafratio = ifelse(is.na(sd_leafratio), 0, sd_leafratio),
         sd_sw = ifelse(is.na(sd_sw), 0, sd_sw),
         sd_st = ifelse(is.na(sd_st), 0, sd_st), 
         sd_volume = ifelse(is.na(sd_volume), 0, sd_volume)) %>% 
  summary()

# Grouping by species; to check to see which traits do not have any intraspecific variation
trait.summary.species <- flamm.df %>%  # scaling so that standard deviations are comparable
  select(fh, species, lfm, leaf_lfm, stem_lfm, mpa, ww_flam_sample, dw_flam_sample, LMA, SLA, thickness, leaf_area, branching, stem_mass_ratio, leaf_mass_ratio, sample_wt, start_temp, branch_volume, ambient_humidity, ambient_temp, thermocoupler_height, hotplate_height) %>% 
  mutate(lfm = scale(lfm), mpa = scale(mpa), ww_flam_sample = scale(ww_flam_sample),
         dw_flam_sample = scale(dw_flam_sample), LMA = scale(LMA), thickness = scale(thickness),
         leaf_area = scale(leaf_area), branching = scale(branching),
         leaf_mass_ratio = scale(leaf_mass_ratio), sample_wt = scale(sample_wt),
         start_temp = scale(start_temp), branch_volume = scale(branch_volume)) %>%  # scaling traits we want to look at so that standard deviations are comparable
  group_by(species) %>% 
  dplyr::summarise(sd_lfm = sd(lfm), sd_mpa = sd(mpa), sd_ww = sd(ww_flam_sample), sd_dw = sd(dw_flam_sample),
                   sd_lma = sd(LMA), sd_thickness = sd(thickness), sd_leafarea = sd(leaf_area),
                   sd_branching = sd(branching), sd_leafratio = sd(leaf_mass_ratio), sd_sw = sd(sample_wt),
                   sd_st = sd(start_temp), sd_volume = sd(branch_volume))
trait.summary.species %>% 
  mutate(sd_lfm = ifelse(is.na(sd_lfm), 0, sd_lfm),
         sd_mpa = ifelse(is.na(sd_mpa), 0, sd_mpa),
         sd_ww = ifelse(is.na(sd_ww), 0, sd_ww),
         sd_dw = ifelse(is.na(sd_dw), 0, sd_dw),
         sd_lma = ifelse(is.na(sd_lma), 0, sd_lma),
         sd_thickness = ifelse(is.na(sd_thickness), 0, sd_thickness),
         sd_leafarea = ifelse(is.na(sd_leafarea), 0, sd_leafarea),
         sd_branching = ifelse(is.na(sd_branching), 0, sd_branching),
         sd_leafratio = ifelse(is.na(sd_leafratio), 0, sd_leafratio),
         sd_sw = ifelse(is.na(sd_sw), 0, sd_sw),
         sd_st = ifelse(is.na(sd_st), 0, sd_st), 
         sd_volume = ifelse(is.na(sd_volume), 0, sd_volume)) %>% 
  summary()
```

The following plant traits are measured for each plant_id (sd = 0 for plant_id):
LFM/stem LFM/leaf LFM, LMA/SLA, thickness, leaf area, leaf ratio/stem ratio

vs. the following traits which have unique values for each plant_id:
MPa, wet weight/dry weight (note: LFM used in calculation), branching, sample weight, starting temp., sample volume

# Example of MEM Selection
Using flame height as the dependent variable, I am going to to show how we could potentially frame our thoughts and what steps we could take in the MEM selection for our larger analysis

## Collinearity Check
Before looking at VIF to check if our models break any collinearity rules (VIF > 5), a visual check of correlations between metrics can identify any obvious problem areas. I am going to include flame height and ALL potential predictors and covariates in the correlation matrix below, although right away we have some expectations about which are going to be colinear (e.g., SLA and LMA).

```{r}
flamm.df <- flamm.df %>% 
  drop_na(fh) # dropping any NA's for flame height

flamm.df %>% 
  select(fh, species, lfm, leaf_lfm, stem_lfm, mpa, ww_flam_sample, dw_flam_sample, LMA, SLA, thickness, leaf_area, branching, stem_mass_ratio, leaf_mass_ratio, sample_wt, start_temp, branch_volume, ambient_humidity, ambient_temp, thermocoupler_height, hotplate_height) %>% 
  ggcorr(label = T)
```

From the above correlation matrix, there are some initial 'problem areas':
- Sample weight and dw_flam_sample and ww_flam_sample: makes sense since sample weight was used in the calculation of these variables
- Branch volume and sample weight, dw_flam_sample, ww_flam_sample: all describe bulkiness, size of sample in some way
- LMA and SLA: duh
- Leaf mass ratio and stem mass ratio: also, duh
- Stem LFM and LFM
- Leaf area and dw_flam_sample and ww_flam_sample: more of a surprising one to me, but makes sense conceptually

So, we will eliminate some variables from contention right away:
- SLA: we can use LMA
- Stem mass ratio: we can use leaf mass ratio
- Stem LFM and leaf LFM: unless we notice something in the EDA that suggests otherwise, I think using total LFM makes sense
- Leaf area: because leaf area is included in some fashion in the calculation of LMA and due to it's oddly strong correlation to dw_flam_sample and ww_flam_sample which we're interested in keeping in the models, it makes sense to remove

We can also eliminate ambient temp and humidity since they show absolutely no correlation with flame height (and very little correlation with most variables)

Just for fun, here's an updated correlation matrix with those variables removed
```{r}
flamm.df %>% 
  filter(start_temp > 0) %>%  # to get rid of -inf values (when datalogger was off)
  select(fh, species, lfm, mpa, ww_flam_sample, dw_flam_sample, LMA, thickness, branching, leaf_mass_ratio, sample_wt, start_temp, branch_volume, thermocoupler_height, hotplate_height) %>% 
  ggcorr(label = T)
```

Currently, there would be 14 predictors and covariates in the model structure which is far too many for an example. I'm going to cut out thickness, start_temp, thermocoupler_height and hotplate_height due to their limited correlations with flame height (note: i could've also removed mpa, but since the hydration question is something we're generally more interested in, it made sense to keep it in there for now). 10 predictors and covariates is a lot, but it is certainly more managable than 14.

We need to do some basic data wrangling before we do anything with lmer
```{r}
# First, fixing 'plant' variable as described in blurb on data structure
flamm.df <- flamm.df %>% 
  unite('plant_id', c(species, plant), sep = '_', remove = F)

# Now scaling all variables
scaled.flamm.df <- flamm.df %>% 
  mutate(fh = scale(fh), lfm = scale(lfm), mpa = scale(mpa), ww_flam_sample = scale(ww_flam_sample), dw_flam_sample = scale(dw_flam_sample), LMA = scale(LMA), branching = scale(branching), leaf_mass_ratio = scale(leaf_mass_ratio), sample_wt = scale(sample_wt), branch_volume = scale(branch_volume))
```

Now, checking VIF scores of 'full model'
```{r}
# Linear mixed effects model structure w/ lmer()
fh_full <- lmer(fh ~ species + lfm + mpa + ww_flam_sample + dw_flam_sample + LMA + branching + leaf_mass_ratio + sample_wt + branch_volume + (1|plant_id), data = scaled.flamm.df)

# Testing multicollinearity w/ performance package
multicollinearity(fh_full) # Ignore the low, moderate, high categories and focus on VIF scores
```

LFM, ww_flam_sample, dw_flam_sample, and species all have VIF > 5. We may not need to remove all of them, but we will need to remove at least some of them. Although we're interested in dw_flam_sample and ww_flam_sample, we learned from above correlation matrices that they are strongly correlated with sample_wt. Let's remove them for now and we can try some models with them in the future to see if they improve the AIC score. Initially, I'm not totally sure why species has such an inflated VIF, so let's take a look at the distribution of the other continuous variables as grouped by species to get a better sense of why this could be.

```{r}
flamm.df %>%
  select(fh, species, lfm, mpa, ww_flam_sample, dw_flam_sample, LMA, branching, leaf_mass_ratio, sample_wt, branch_volume) %>% 
  pivot_longer(cols = !c(species), names_to = 'variable', values_to = 'value') %>% 
  ggplot(aes(x = species, y = value, color = species)) +
    geom_boxplot(alpha = 0.8) +
    facet_wrap(~variable, scales = 'free') +
    labs(x = 'Species', y = 'Value') +
    theme_bw() +
    theme(legend.position = 'none',
          axis.title = element_text(face = 'bold', size = 14),
          axis.text.x = element_text(angle = 15, size = 10)) 
```
 
It looks like LMA (with the exception of ERIKAR) and leaf_mass_ratio have very little within-species variation in this dataset, so this could be a reason why it is getting flagged. Let's keep species in for now and remove LMA and/or leaf_mass_ratio and see how that looks.

Trying out some model structures to try to eliminate multicollinearity while keeping species in the model
```{r}
fh_m1 <- lmer(fh ~ species + lfm + mpa + LMA + branching + sample_wt + branch_volume + (1|plant_id), data = scaled.flamm.df) # removed dw, ww, leaf mass ratio and kept in LMA
multicollinearity(fh_m1) # species and lfm still VIF > 5

fh_m2 <- lmer(fh ~ species + lfm + mpa + branching + sample_wt + branch_volume + (1|plant_id), data = scaled.flamm.df) # removed dw, ww, leaf mass ratio and LMA
multicollinearity(fh_m2) # species and lfm still VIF > 5

fh_m3 <- lmer(fh ~ species + mpa + branching + sample_wt + branch_volume + (1|plant_id), data = scaled.flamm.df) # removed dw, ww, leaf mass ratio, LMA, and lfm
multicollinearity(fh_m3) # no multicollinearity (removing lfm seemed to do the trick)

fh_m4 <- lmer(fh ~ species + mpa + LMA + branching + sample_wt + branch_volume + (1|plant_id), data = scaled.flamm.df) # removed dw, ww, leaf mass ratio, and lfm
multicollinearity(fh_m4) # adding back in LMA made species VIF higher

fh_m5 <- lmer(fh ~ species + mpa + leaf_mass_ratio + branching + sample_wt + branch_volume + (1|plant_id), data = scaled.flamm.df) # removed dw, ww, LMA, and lfm
multicollinearity(fh_m5) # adding back in leaf mass ratio had less of an effect than adding LMA, but still made species VIF higher

fh_m6 <- lmer(fh ~ species + mpa + branching + branch_volume + (1|plant_id), data = scaled.flamm.df) # removed dw, ww, leaf mass ratio, LMA, sample weight and lfm
multicollinearity(fh_m6) # no multicollinearity; sample weight may have been tied to species

# Trying to re-add other variables to see how it affects species VIF
fh_m7 <- lmer(fh ~ species + mpa + LMA + branching + branch_volume + (1|plant_id), data = scaled.flamm.df) # removed dw, ww, leaf mass ratio, sample weight and lfm
multicollinearity(fh_m7) # species VIF > 5

fh_m8 <- lmer(fh ~ species + mpa + leaf_mass_ratio + branching + branch_volume + (1|plant_id), data = scaled.flamm.df) # removed dw, ww, LMA, sample weight and lfm
multicollinearity(fh_m8) # species VIF > 5

fh_m9 <- lmer(fh ~ species + mpa + lfm + leaf_mass_ratio + branching + branch_volume + (1|plant_id), data = scaled.flamm.df) # removed dw, ww, LMA, and sample weight
multicollinearity(fh_m9) # species and lfm both VIF > 5

fh_m10 <- lmer(fh ~ species + mpa + dw_flam_sample + leaf_mass_ratio + branching + branch_volume + (1|plant_id), data = scaled.flamm.df) # removed ww, LMA, sample weight and lfm
multicollinearity(fh_m10) # Species VIF > 5

fh_m11 <- lmer(fh ~ species + mpa + ww_flam_sample + leaf_mass_ratio + branching + branch_volume + (1|plant_id), data = scaled.flamm.df) # removed dw, LMA, sample weight and lfm
multicollinearity(fh_m11) # Species VIF > 5
```

Okay, it seems like if we keep species in, we'll have to make some significant sacrifices elsewhere in the model to avoid breaking multicollinearity assumptions. Let's try some model structures w/ removing species:
```{r}
fh_m12 <- lmer(fh ~ lfm + mpa + ww_flam_sample + dw_flam_sample + LMA + branching + leaf_mass_ratio + sample_wt + branch_volume + (1|plant_id), data = scaled.flamm.df)
multicollinearity(fh_m12) # ww and dw are collinear

fh_m13 <- lmer(fh ~ lfm + mpa + LMA + branching + leaf_mass_ratio + sample_wt + branch_volume + (1|plant_id), data = scaled.flamm.df) # no ww or dw
multicollinearity(fh_m13) # no multicollinearity

fh_m14 <- lmer(fh ~ mpa + lfm  + dw_flam_sample + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df) # no ww or sample wt
multicollinearity(fh_m14) # no multicollinearity

fh_m15 <- lmer(fh ~ mpa + lfm  + ww_flam_sample + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df) # no ww or sample wt
multicollinearity(fh_m15) # no multicollinearity

fh_m16 <- lmer(fh ~ mpa + dw_flam_sample + ww_flam_sample + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df)
multicollinearity(fh_m16) # ww and dw are collinear
```

Without species in the model, we can do a lot more w/ the model structure. Multicollinearity becomes an issue with LFM, DW, WW, and sample weight. DW or WW and LFM can be in the same model (see fh_m14 and fh_m15), but DW, WW, and sample_wt all need to be considered in separate models. Now we have a decent understanding of which variables could be included in the more in-depth model selection

## Interaction Check
Although I don't plan on including interaction terms in the AIC-based selection (below) for this example, as it would make the selection even more complicated than it already is, it is worth checking to see whether there is evidence that there should/could be interaction terms included in the fixed and/or random effects. I am also using this chunk as a trial of interaction.plot()
```{r}
with(scaled.flamm.df, {
  interaction.plot(species, plant, fh)
  interaction.plot(species, plant, fh)})
```


## AIC-Based Selection
We have been focused on using AIC to select the 'best' models in the mixed effects model selection. Below, I tried all of the permutations w/o interactions to see which predictor variables are selected in the top models. Interaction terms with species may be important in the final model selection, but to make things a bit simpler for this example, I decided against using any.

Also, note that I started over at fh_m1 below (throwing coding best practices out the window), but that the below models in no way relate to the multicollinearity check above

Starting w/ models with species in, we have mpa, leaf_mass_ratio, branching and branch_volume in the model without multicollinearity with the most variables. Assuming we want to keep species in the model, there are 4 variables that could be arranged in 15 different ways. I also tried some models with just species and a single other variable (e.g., LMA, lfm, dw) to see if that'd work
```{r, results = 'hide'}
fh_m1 <- lmer(fh ~ species + mpa + leaf_mass_ratio + branching + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m1) # VIF > 5 for species (note, changed REML = T to double-check and it was still VIF > 5)

fh_m2 <- lmer(fh ~ species + mpa + branching + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m2)

fh_m3 <- lmer(fh ~ species + mpa + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m3)

fh_m4 <- lmer(fh ~ species + mpa + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m4)

fh_m5 <- lmer(fh ~ species + branching + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m5)

fh_m6 <- lmer(fh ~ species + mpa + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m6)

fh_m7 <- lmer(fh ~ species + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m7)

fh_m8 <- lmer(fh ~ species + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m8)

fh_m9 <- lmer(fh ~ species + leaf_mass_ratio + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m9)

fh_m10 <- lmer(fh ~ species + mpa + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m10)

fh_m11 <- lmer(fh ~ species + mpa + leaf_mass_ratio + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m11)

fh_m12 <- lmer(fh ~ species + leaf_mass_ratio + branching + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = T)
multicollinearity(fh_m12) # VIF > 5 for species (note, changed REML = T to double-check and it was still VIF > 5)

fh_m13 <- lmer(fh ~ species + mpa + leaf_mass_ratio + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m13)

fh_m14 <- lmer(fh ~ species + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m14)

fh_m15 <- lmer(fh ~ species + leaf_mass_ratio + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m15)

fh_m16 <- lmer(fh ~ species + lfm + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m16) # breaks multicollinearity assumption

fh_m17 <- lmer(fh ~ species + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m17)

fh_m18 <- lmer(fh ~ species + dw_flam_sample + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m18)

fh_m19 <- lmer(fh ~ species + ww_flam_sample + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m19)

fh_m20 <- lmer(fh ~ species + sample_wt + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m20)
```

```{r}
tab_model(fh_m2, fh_m3, fh_m4, fh_m5, fh_m6, fh_m7, fh_m8, fh_m9, fh_m10,
          fh_m11, fh_m13, fh_m14, fh_m15, fh_m17, fh_m18, fh_m19, fh_m20,
          show.reflvl = TRUE, 
          digits = 3, 
          show.aic = TRUE, 
          show.ci = FALSE,
          show.icc = FALSE, 
          string.pred = "Coeffcient", 
         title = "MEM Selection: Flame Height, Species in Models",
  string.p = "P-Value", 
  p.style = "stars")
?tab_model
```

From the above model selection, dw_flam_sample, ww_flam_sample and sample weight emerged as key predictors when paired w/ species. 

Before we delve into that a bit further (to see which other variables we could potentially pair with dw or ww in a model), let's do a selection without species. For this selection, since we have 9 predictor variables (although some of them cannot be paired with each other (see multicollinearity notes on LFM, dw, ww, sample weight above)), we will have to approach it a different way, or else we'd have WAY too many possible permutations of predictors. In this model selection, the idea is to whittle down the 'bad' predictors so that we can get to a manageable level of predictors.

```{r, results = 'hide'}
fh_m21 <- lmer(fh ~ lfm + mpa + dw_flam_sample + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m21)

fh_m22 <- lmer(fh ~ lfm + mpa + sample_wt + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m22)

fh_m23 <- lmer(fh ~ lfm + mpa + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m23)

# Removing LFM
fh_m24 <- lmer(fh ~ mpa + dw_flam_sample + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m24)

fh_m25 <- lmer(fh ~ mpa + sample_wt + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m25)

fh_m26 <- lmer(fh ~ mpa + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m26)

# Removing MPa
fh_m27 <- lmer(fh ~ lfm + dw_flam_sample + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m27)

fh_m28 <- lmer(fh ~ lfm + sample_wt + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m28)

fh_m29 <- lmer(fh ~ lfm + LMA + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m29)

# Removing LMA
fh_m30 <- lmer(fh ~ lfm + mpa + dw_flam_sample + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m30)

fh_m31 <- lmer(fh ~ lfm + mpa + sample_wt + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m31)

fh_m32 <- lmer(fh ~ lfm + mpa + branching + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m32)

# Removing branching
fh_m33 <- lmer(fh ~ lfm + mpa + dw_flam_sample + LMA + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m33)

fh_m34 <- lmer(fh ~ lfm + mpa + sample_wt + LMA + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m34)

fh_m35 <- lmer(fh ~ lfm + mpa + LMA + leaf_mass_ratio + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m35)

# Removing leaf mass ratio
fh_m36 <- lmer(fh ~ lfm + mpa + dw_flam_sample + LMA + branching + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m36)

fh_m37 <- lmer(fh ~ lfm + mpa + sample_wt + LMA + branching + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m37)

fh_m38 <- lmer(fh ~ lfm + mpa + LMA + branching + branch_volume + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m38)

# Removing volume
fh_m39 <- lmer(fh ~ lfm + mpa + dw_flam_sample + LMA + branching + leaf_mass_ratio + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m39)

fh_m40 <- lmer(fh ~ lfm + mpa + sample_wt + LMA + branching + leaf_mass_ratio + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m40)

fh_m41 <- lmer(fh ~ lfm + mpa + LMA + branching + leaf_mass_ratio + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m41)
```

```{r}
tab_model(fh_m21, fh_m22, fh_m23, fh_m24, fh_m25, fh_m26, fh_m27, fh_m28, fh_m29, fh_m30,
          fh_m31, fh_m32, fh_m33, fh_m34, fh_m35, fh_m36, fh_m37, fh_m38, fh_m39, fh_m40,
          fh_m41,
          show.reflvl = TRUE, 
          digits = 3, 
          dv.labels = c(rep('full', 3), rep('no lfm', 3), rep('no mpa', 3), rep('no lma', 3), 
                        rep('no branching', 3), rep('no leaf mass ratio', 3), rep('no volume', 3)),
          show.aic = TRUE, 
          show.ci = FALSE,
          show.icc = FALSE, 
          string.pred = "Coeffcient", 
         title = "MEM Selection: Flame Height, No Species in Models",
  string.p = "P-Value", 
  p.style = "stars")
```

I interpretted the above model selection table as follows: when there is a decrease in the AIC score after removing a variable, that variable makes the overall model fit worse, but when there is an increase in the AIC score after removing a variable, that variable makes the overall model fit better. With that in mind, here's what we see:
- LFM: generally makes model fit worse (AIC -4 when removed), but conceptually we're interested in it, so we'll leave it in for now (but if wet weight or mpa is present in the model, then we don't need to include mpa)
- MPa: again, generally makes model fit worse (AIC -3 when removed), but conceptually we're interested in it, so we'll leave it in for now (but if wet weight or LFM is present in the model, then we don't need to include mpa)
- Volume: if ww, dw, or sample weight is present, makes model fit worse, but if those are not in the model, then it makes model fit better
- LMA: keep in! Removing this variable dramatically made the model fit worse
- Branching: it doesn't make a huge difference on model fit; keep in
- Leaf mass ratio: generally makes model fit worse, so we can remove

Okay, that significantly narrows down the possible options for the model structure. Let's do another table with that new knowledge. Here are the 28 model structures that seem ideal: 
- 1. hydration (LFM or MPa) + sample size (DW, sample wt or volume) + LMA + branching (6)
- 2. hydration (LFM or MPa) + sample size (DW, sample wt or volume) + LMA (6)
- 3. sample size (WW, DW, sample wt or volume) + LMA + branching (4)
- 4. sample size (WW, DW, sample wt or volume) + LMA (4)
- 5. hydration (WW, LFM or MPa) + LMA + branching (3)
- 6. hydration (WW, LFM or MPa) + LMA (3)
- 7. LMA + branching (1)
- 8. LMA (1)

where hydration = LFM, MPa or wet weight and sample size = wet weight, dry weight, sample weight or sample volume, but wet weight should not be included in models with LFM or MPa, nor should it be included in models with dry weight or sample weight.

```{r, results = 'hide'}
# 1.
fh_m42 <- lmer(fh ~ lfm + dw_flam_sample + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m42)

fh_m43 <- lmer(fh ~ lfm + sample_wt + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m43)

fh_m44 <- lmer(fh ~ lfm + branch_volume + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m44)

fh_m45 <- lmer(fh ~ mpa + dw_flam_sample + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m45)

fh_m46 <- lmer(fh ~ mpa + sample_wt + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m46)

fh_m47 <- lmer(fh ~ mpa + branch_volume + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m47)

# 2. 
fh_m48 <- lmer(fh ~ lfm + dw_flam_sample + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m48)

fh_m49 <- lmer(fh ~ lfm + sample_wt + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m49)

fh_m50 <- lmer(fh ~ lfm + branch_volume + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m50)

fh_m51 <- lmer(fh ~ mpa + dw_flam_sample + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m51)

fh_m52 <- lmer(fh ~ mpa + sample_wt + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m52)

fh_m53 <- lmer(fh ~ mpa + branch_volume + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m53)

# 3. 
fh_m54 <- lmer(fh ~ dw_flam_sample + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m54)

fh_m55 <- lmer(fh ~ sample_wt + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m55)

fh_m56 <- lmer(fh ~ branch_volume + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m56)

fh_m57 <- lmer(fh ~ ww_flam_sample + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m57)

# 4. 
fh_m58 <- lmer(fh ~ dw_flam_sample + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m58)

fh_m59 <- lmer(fh ~ sample_wt + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m59)

fh_m60 <- lmer(fh ~ branch_volume + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m60)

fh_m61 <- lmer(fh ~ ww_flam_sample + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m61)

# 5. 
fh_m62 <- lmer(fh ~ ww_flam_sample + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m62) # note: whoops, same as fh_m57

fh_m63 <- lmer(fh ~ mpa + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m63)

fh_m64 <- lmer(fh ~ lfm + LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m64)

# 6.
fh_m65 <- lmer(fh ~ ww_flam_sample + LMA + (1|plant_id), data = scaled.flamm.df, REML = F) # note: whoops, same as fh_m61
multicollinearity(fh_m65) 

fh_m66 <- lmer(fh ~ mpa + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m66)

fh_m67 <- lmer(fh ~ lfm + LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m67)

# 7.
fh_m68 <- lmer(fh ~ LMA + branching + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m68)

# 8. 
fh_m69 <- lmer(fh ~ LMA + (1|plant_id), data = scaled.flamm.df, REML = F)
multicollinearity(fh_m69)

# Adding interaction terms to random effect for top 2 models
fh_int1 <- lmer(fh ~ ww_flam_sample + LMA + branching + (1|plant_id:species), data = scaled.flamm.df, REML = F)

fh_int2 <- lmer(fh ~ ww_flam_sample + LMA + (1|plant_id:species), data = scaled.flamm.df, REML = F)
```

Alright, let's add a couple of the best models identified in the table w/ species included and then do a model selection table
```{r}
tab_model(fh_m18, fh_m19, fh_m42, fh_m43, fh_m44, fh_m45, fh_m46, fh_m47, fh_m48, fh_m49,
          fh_m50, fh_m51, fh_m52, fh_m53, fh_m54, fh_m55, fh_m56, fh_m57, fh_m58, fh_m59,
          fh_m60, fh_m61, fh_m63, fh_m64, fh_m66, fh_m67, fh_m68, fh_m69, fh_int1, fh_int2,
          show.reflvl = TRUE, 
          digits = 3, 
          dv.labels = c(rep('w/ species', 2), rep('hydration + sample size + LMA + branching', 6),
                        rep('hydration +sample size + LMA', 6), rep('sample size + LMA + branching', 4),
                        rep('sample size + LMA', 4), rep('hydration + LMA + branching', 2),
                        rep('hydration + LMA', 2), 'LMA + branching', 'LMA', 'Interaction 1', 'Interaction 2'),
          show.aic = TRUE, 
          show.ci = FALSE,
          show.icc = FALSE, 
          string.pred = "Coeffcient", 
         title = "MEM Selection: Flame Height, Penultimate Model Selection",
  string.p = "P-Value", 
  p.style = "stars")
```

Alright, we're close to being done now. Here's what we learned from the above table:
- ww_flam_sample is the 'best' sample size predictor, followed up by sample_wt; branch_volume is the 'worst' sample size predictor
- MPa seems to be a better predictor than LFM (this is slightly unexpected based on other EDA)
- Branching seems to slightly improve the model fit, but the effect varies
- interaction term w/ species for the random effect has no effect on AIC (maybe try AICc)

Here are the top 5 models and their respective (rounded) AIC scores:
- fh_int1 or fh_57: ww_flam_sample + LMA + branching (interaction in random effect or not; same AIC), AIC = 322.6
- fh_m46: MPa + sample_wt + LMA + branching, AIC = 323.0
- fh_m55: sample_wt + LMA + branching, AIC = 323.3
- fh_m52: MPa + sample_wt + LMA, AIC = 323.8
- fh_m43: LFM + sample_wt + LMA + branching, AIC = 325.1

Let's see if removing LMA improves the model fit for any of the above models (gut check)
```{r}
fh_m46_noLMA <- lmer(fh ~ mpa + sample_wt + branching + (1|plant_id), data = scaled.flamm.df, REML = F)

fh_int1_noLMA <- lmer(fh ~ ww_flam_sample + branching + (1|plant_id:species), data = scaled.flamm.df, REML = F)

fh_m55_noLMA <- lmer(fh ~ sample_wt + branching + (1|plant_id), data = scaled.flamm.df, REML = F)

fh_m52_noLMA <- lmer(fh ~ mpa + sample_wt + (1|plant_id), data = scaled.flamm.df, REML = F)

fh_m43_noLMA <- lmer(fh ~ lfm + sample_wt + branching + (1|plant_id), data = scaled.flamm.df, REML = F)

tab_model(fh_int1, fh_int1_noLMA, fh_m46, fh_m46_noLMA, fh_m55, fh_m55_noLMA, 
          fh_m52, fh_m52_noLMA, fh_m43, fh_m43_noLMA,
          show.reflvl = TRUE, 
          digits = 3,
          show.aic = TRUE, 
          show.ci = FALSE,
          show.icc = TRUE, 
          string.pred = "Coeffcient", 
         title = "MEM Selection: Flame Height, Removing LMA",
  string.p = "P-Value", 
  p.style = "stars")

anova(fh_int1, fh_m46, fh_m55, fh_m52, fh_m43)
```
As expected, removing LMA worsened these models in every case

Let's see if interaction term in the random effect consistently improves model fit -- using CONDITIONAL AIC (AICc)
```{r}
#install.packages('cAIC4')
library(cAIC4)

fh_int46 <- lmer(fh ~ mpa + sample_wt + LMA + branching + (1|plant_id:species), data = scaled.flamm.df, REML = F)

fh_int55 <- lmer(fh ~ sample_wt + LMA + branching + (1|plant_id:species), data = scaled.flamm.df, REML = F)

fh_int52 <- lmer(fh ~ mpa + sample_wt + LMA + (1|plant_id:species), data = scaled.flamm.df, REML = F)

fh_int43 <- lmer(fh ~ lfm + sample_wt + LMA + branching + (1|plant_id:species), data = scaled.flamm.df, REML = F)

models <- c('fh_m57', 'fh_int1', 'fh_m46', 'fh_int46', 'fh_m55', 'fh_int55', 
            'fh_m52', 'fh_int52', 'fh_m43', 'fh_int43')
cond.AIC <- c(cAIC(fh_m57)[[5]], cAIC(fh_int1)[[5]], cAIC(fh_m46)[[5]], cAIC(fh_int46)[[5]],
              cAIC(fh_m55)[[5]], cAIC(fh_int55)[[5]], cAIC(fh_m52)[[5]], cAIC(fh_int52)[[5]],
              cAIC(fh_m43)[[5]], cAIC(fh_int43)[[5]])
df.cAIC <- data.frame(models, cond.AIC)
```
Hmmm, with conditional AIC, interaction term does not affect model fit at all. New model was selected as significantly best model though (fh_m46 or fh_int46)

## P-Values
Because the stars reported in the model table are not totally accurate, we should check out the Kenward-Roger approximations of p-values for each of the top two models
```{r}
fh_top1 <- lmer(fh ~ mpa + sample_wt + LMA + branching + (1|plant_id), data = scaled.flamm.df) # REML must = T for Kenward-Roger approx. p-values
summary(fh_top1, ddf = 'Kenward-Roger')

fh_top2 <- lmer(fh ~ ww_flam_sample + LMA + branching + (1|plant_id), data = scaled.flamm.df)
# REML must = T for Kenward-Roger approx. p-values
summary(fh_top2, ddf = 'Kenward-Roger')

fh_top3 <- lmer(fh ~ sample_wt + LMA + branching + (1|plant_id), data = scaled.flamm.df)
# REML must = T for Kenward-Roger approx. p-values
summary(fh_top3, ddf = 'Kenward-Roger')

fh_top4 <- lmer(fh ~ mpa + sample_wt + LMA + (1|plant_id), data = scaled.flamm.df)
# REML must = T for Kenward-Roger approx. p-values
summary(fh_top4, ddf = 'Kenward-Roger')
```

For all four of the models investigated, all of the included fixed effects except for LMA are significant!

## Checking Assumptions (more thoroughly)
Following environmental computing tutorial
```{r}
# lmer removes NAs but our linear model in the ANOVA, so we'll have to remove the NA's
scaled.flamm.df.noNAs <- scaled.flamm.df %>% 
  select(fh, species, lfm, mpa, ww_flam_sample, dw_flam_sample, sample_wt, leaf_mass_ratio, branch_volume, LMA, branching, plant_id) %>% 
  na.omit() # this removed 13 samples

# Model
fh_top1 <- lmer(fh ~ mpa + sample_wt + LMA + branching + (1|plant_id), data = scaled.flamm.df.noNAs, REML = F)

#Checking Assumptions
#Assumption 1: Observed y are independent, randomly sampled
#Assumption 2: Residuals are normally distributed - use qqplot
fh_top1.res <- residuals(fh_top1)
qqPlot(fh_top1.res) #From package CAR, looks good to me
#Assumptions 3 and 4: Residuals have constant variance; and there is a straight line relationship between y and x and random effects (z) - use residuals vs. fitted plot
plot(fh_top1)
#Assumption 5: Random effects are independent of y - random sampling
#Assumption 6: Random effects are normally distributed - tough to check

#Test effects of fixed effect with ANOVA
fh_top1.0 <- lmer(fh ~ (1|plant_id), data = scaled.flamm.df.noNAs, REML = F)
anova(fh_top1, fh_top1.0) #P-value shows that fixed effects have significant effect on the y

#Test effects of random effect with parametric bootstrap method
nBoot=1000
lrStat=rep(NA,nBoot)
fh_null <- lm(fh ~ mpa + sample_wt + LMA + branching, data = scaled.flamm.df.noNAs) # null model
fh_alt <- lmer(fh ~ mpa + sample_wt + LMA + branching + (1|plant_id), data = scaled.flamm.df.noNAs, REML = F) # alternate model
lrObs <- 2*logLik(fh_alt) - 2*logLik(fh_null) # observed test stat
for(iBoot in 1:nBoot)
{
  scaled.flamm.df.noNAs$fhSim=unlist(simulate(fh_null)) #resampled data
  bNull <- lm(fh ~ ww_flam_sample + LMA + branching, data = scaled.flamm.df.noNAs) # null model
  bAlt <- lmer(fh ~ ww_flam_sample + LMA + branching + (1|plant_id), data = scaled.flamm.df.noNAs, REML = F) # alternate model
  lrStat[iBoot] <- 2*logLik(bAlt) - 2*logLik(bNull) # resampled test stat
}
mean(lrStat>lrObs) #P-value for test of plant_id effect
```


## Alternative Process (Note)
There are other functions that try and get the 'best' model for the data by comparing p-values and discarding multicollinear variables and such; however, in my experience, these functions are flawed, as (1) they often do not select the 'best' model, (2) the user cannot see the processes driving the model selection and cannot tweak the model selection according to their questions or the EDA, and (3) it does not take into account multicollinearity
```{r}
# step() is very sensitive to NAs, so we'll have to remove the NA's
scaled.flamm.df.noNAs <- scaled.flamm.df %>% 
  select(fh, species, lfm, mpa, ww_flam_sample, dw_flam_sample, sample_wt, leaf_mass_ratio, branch_volume, LMA, branching, plant_id) %>% 
  na.omit() # this removed 13 samples

# For the first example, let's throw the kitchen sink at this function and see what it spits out
fh_step <- lmer(fh ~ species + lfm + mpa + ww_flam_sample + dw_flam_sample + sample_wt + branch_volume + LMA + branching + (1|plant_id), data = scaled.flamm.df.noNAs)
# Stepwise model selection
step_res <- step(fh_step, ddf = 'Kenward-Roger')
final <- get_model(step_res)
# Checking out selected model
summary(final)
multicollinearity(final) # step() does not take into account multicollinearity, so the 'final' model is invalid in this example

# For the second example, let's throw the identified 'best' model from above and species and see what it says
fh_step2 <- lmer(fh ~ mpa + sample_wt + LMA + branching + (1|plant_id:species), data = scaled.flamm.df.noNAs)
# Stepwise model selection
step_res <- step(fh_step2, ddf = 'Kenward-Roger')
final <- get_model(step_res)
# Checking out selected model
summary(final)
multicollinearity(final)

# Comparing to top model selected above:
fh_step_selected <- lmer(fh ~ mpa + sample_wt + branching + (1|plant_id:species), data = scaled.flamm.df) 
# Using full dataset now
AIC(fh_step_selected)
AIC(fh_top1)
# The step-selected is significantly worse based on AIC score; now, maybe this is because of the NAs reintroduced into the data, so this may not be a totally fair comparison.
```

## Visualizations via REMEF

### MPa vs. FH
```{r}
fh_top1 <- lmer(fh ~ mpa + sample_wt + LMA + branching + (1|plant_id), data = scaled.flamm.df.noNAs, REML = F) # remef is also sensitive to NAs

scaled.flamm.df.noNAs$r_fh <- remef(fh_top1, fix = c('LMA', 'branching', 'sample_wt'), ran = "all", keep.intercept = FALSE) # w/ remef, we are removing the fixed effects of LMA, sample weight and branching and removing the random effects to isolate the relationship between mpa and flame height

# With REMEF
ggplot(scaled.flamm.df.noNAs, aes(x = mpa, y = r_fh))  +
  geom_point(alpha = 0.8, color = 'gray20') +
  geom_smooth(method = 'lm', color = 'gray40') +
  stat_cor(label.y = 0.9)+ 
  stat_regline_equation(label.y = 1.2) +
  labs(x = 'Water Potential (scaled)', y = 'Flame Height (effects removed, scaled)') +
  theme_bw() +
  theme(axis.title = element_text(face = 'bold', size = 15),
        axis.text = element_text(size = 12))

# Without REMEF
ggplot(scaled.flamm.df.noNAs, aes(x = mpa, y = fh))  +
  geom_point(alpha = 0.8, color = 'gray20') +
  geom_smooth(method = 'lm', color = 'gray40') +
  stat_cor(label.y = 3.5)+ 
  stat_regline_equation(label.y = 4) +
  labs(x = 'Water Potential (scaled)', y = 'Flame Height (scaled)') +
  theme_bw() +
  theme(axis.title = element_text(face = 'bold', size = 15),
        axis.text = element_text(size = 12))
```

### Sample Wt. vs. FH
```{r}
scaled.flamm.df.noNAs$r_fh2 <- remef(fh_top1, fix = c('LMA', 'branching', 'mpa'), ran = "all", keep.intercept = FALSE) # w/ remef, we are removing the fixed effects of LMA, mpa and branching and removing the random effects to isolate the relationship between sample weight and flame height

# With REMEF
ggplot(scaled.flamm.df.noNAs, aes(x = sample_wt, y = r_fh2))  +
  geom_point(alpha = 0.8, color = 'gray20') +
  geom_smooth(method = 'lm', color = 'gray40') +
  stat_cor(label.y = 2)+ 
  stat_regline_equation(label.y = 2.5) +
  labs(x = 'Sample Weight (scaled)', y = 'Flame Height (effects removed, scaled)') +
  theme_bw() +
  theme(axis.title = element_text(face = 'bold', size = 15),
        axis.text = element_text(size = 12))

# Without REMEF
ggplot(scaled.flamm.df.noNAs, aes(x = sample_wt, y = fh))  +
  geom_point(alpha = 0.8, color = 'gray20') +
  geom_smooth(method = 'lm', color = 'gray40') +
  stat_cor(label.y = 3.5)+ 
  stat_regline_equation(label.y = 4) +
  labs(x = 'Sample Weight (scaled)', y = 'Flame Height (scaled)') +
  theme_bw() +
  theme(axis.title = element_text(face = 'bold', size = 15),
        axis.text = element_text(size = 12))
```


# -------------------------------

# Mixed Effects LASSO
The below code is meant to serve as (a) practice using the mixed effects LASSO technique and (b) an example to compare with the AIC-based approach outlined above.

## Setup/Neceessary Packages
```{r}
library(glmmLasso) # specific to running lasso on generalized linear mixed models
library(glmnet) # 'best' package for running lasso on generalized linear models; also, has good function for cross-validation process
```
## Example 1: Linear Model
(trying it out w/o random effects)
```{r}
# to set up values to check for lambda (for now, will do cross-validation later)
grid <- 10^seq(10, -2, length = 100)

# setting up model matrices
x <- scaled.flamm.df.noNAs %>% 
  select(lfm, mpa, ww_flam_sample, dw_flam_sample, sample_wt, branch_volume, LMA, branching) %>% 
  as.matrix() # for first, iteration, throwing *most* of the kitchen sink into the model (but no species)
y <- scaled.flamm.df.noNAs$fh

lasso.lm <- glmnet(x, y, alpha = 1, lambda = grid)
plot(lasso.lm)
```

Cross-validation process for lambda
```{r}
# splitting dataset into training and test dataset
set.seed(16) # so it's reproducible
train <- sample(c(TRUE, FALSE), nrow(scaled.flamm.df.noNAs), replace = TRUE)
test <- !train
y.test <- y[test]

# 1 iteration of cross-validation
cv.out <- cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
best.lam <- cv.out$lambda.min
best.lam

# 100 iterations of cross-validation
best.lam.list <- list(c(rep(NA, 100))) # creating holding list for best lamda
for(i in 1:100) {
  set.seed(i)
  cv.out <- cv.glmnet(x[train,], y[train], alpha = 1)
  best.lam.list[[1]][i]<- cv.out$lambda.min
}
best.lam100 <- mean(best.lam.list[[1]])
best.lam100
```

```{r}
lasso.lm <- glmnet(x, y, alpha = 1, lambda = best.lam100)
predict(lasso.lm, type = "coefficients", s = best.lam100)[1:9,]
```

## Example 2: Linear Mixed Effects Model

### Cross-Validation for Lambda
from https://davidabugaber.com/blog/f/find-the-optimal-mixed-model-for-your-data-with-glmmlasso
```{r}
#this loop takes much longer to run because there's more random intercepts
 
#all variables should be either continuous/numerical or factors
summary(scaled.flamm.df.noNAs)
scaled.flamm.df.noNAs <- scaled.flamm.df.noNAs %>% 
  mutate(species = as.factor(species))

#set lambdas... go from 10^-5 to 10^5, in 20 log steps (might take a while)
lambda <- 10^seq(-5,5, length=20)
 
#dummy vectors of model fit values for each lambda: BIC, AIC, prediction error
BIC_vec <- rep(Inf, length(lambda))
AIC_vec <- rep(Inf, length(lambda))
Devianz_ma<-NULL
Coeff_ma<-NULL
 
family = gaussian(link = "identity")
 
j<-1
for (j in 1:length(BIC_vec)){
 print(paste("Iteration ", j, sep=""))
 
 glm1 <- try( #throwing kitchen sink in; for now, just using plant.id as random effect
 glmmLasso(fh ~ species + lfm + mpa + ww_flam_sample + dw_flam_sample + sample_wt +
             branch_volume + LMA + branching,
 data=scaled.flamm.df.noNAs,
 rnd = list(plant_id=~1), 
 family = gaussian(link = "identity"),
 lambda = lambda[j],
 switch.NR = TRUE,
 final.re = TRUE), 
 silent = TRUE)
 

# code to make it continue anyway if an error occurs
  if(class(glm1)!="try-error")
 { 
 
 #save BIC, AIC
 BIC_vec[j]<-glm1$bic
 AIC_vec[j]<-glm1$aic
 
 #save coefficient outputs
 Coeff_ma<-cbind(Coeff_ma,glm1$coefficients)
 
 #save error (deviance) values
 y.hat<-predict(glm1,scaled.flamm.df.noNAs) 
 Devianz_ma[j]<-sum(family$dev.resids(scaled.flamm.df.noNAs$fh,y.hat,wt=rep(1,length(y.hat)))) } }

# Best lamdas, based on different criteria
lambda[which.min(BIC_vec)]
# 69.51928

lambda[which.min(AIC_vec)]
# 0.5455595

lambda[which.min(Devianz_ma)]
# 233.5721

# They're all different, so going to try LASSO with each of them
bic.lambda <- lambda[which.min(BIC_vec)] 
aic.lambda <- lambda[which.min(AIC_vec)] 
devianz.lambda <- lambda[which.min(Devianz_ma)] 
```

### LASSO/Model Selection

#### BIC Lambda (intermediate)
```{r}
fh_bic_lasso <- glmmLasso(fh ~ species + lfm + mpa + ww_flam_sample + dw_flam_sample + 
             sample_wt + branch_volume + LMA + branching,
 data=scaled.flamm.df.noNAs,
 rnd = list(plant_id=~1), 
 family = gaussian(link = "identity"),
 lambda = bic.lambda,
 switch.NR = TRUE,
 final.re = TRUE)

summary(fh_bic_lasso)
```

#### AIC Lambda (low)
```{r}
fh_aic_lasso <- glmmLasso(fh ~ species + lfm + mpa + ww_flam_sample + dw_flam_sample + 
             sample_wt + branch_volume + LMA + branching,
 data=scaled.flamm.df.noNAs,
 rnd = list(plant_id=~1), 
 family = gaussian(link = "identity"),
 lambda = aic.lambda,
 switch.NR = TRUE,
 final.re = TRUE)

summary(fh_aic_lasso)
```

#### Devianz Lambda (high)
```{r}
fh_dev_lasso <- glmmLasso(fh ~ species + lfm + mpa + ww_flam_sample + dw_flam_sample + 
             sample_wt + branch_volume + LMA + branching,
 data=scaled.flamm.df.noNAs,
 rnd = list(plant_id=~1), 
 family = gaussian(link = "identity"),
 lambda = devianz.lambda,
 switch.NR = TRUE,
 final.re = TRUE)

summary(fh_dev_lasso)
```

# --------------------------------
# Mallows' CP
Since Mallows' CP is a variant of AIC, we can use an adjusted format of AIC to calculate Mallows' CP
```{r}
mallows.cp <- function(model, k, n = nrow(scaled.flamm.df.noNAs)) {
  return(AIC(model) + 2*k / (n - k - 1))
}

summary(fh_top1)
BIC(fh_top1)
AIC(fh_top1)
mallows.cp(fh_top1, k = 5)
```

# --------

# MODEL SELECTION - OPTIMIZED
I think we can optimize a AIC, BIC, mallows' Cp based model selection limiting the amount of user input

Things to consider:
- Multicollinearity
- *Random Effect Structure!!*

FIRST: List models
```{r, warning = F}
scaled.flamm.df.noNAs.predictors <- scaled.flamm.df.noNAs %>% 
  select(lfm, mpa, ww_flam_sample, dw_flam_sample, sample_wt, leaf_mass_ratio, branch_volume, LMA, branching) # select all predictors that you're interested in looking at

# empty list
mod.list <- list()
call.vec <- c()
# setting up a nested for loop to list all possible models including those predictors
for(i in 2:ncol(scaled.flamm.df.noNAs.predictors)){
  call <- colnames(scaled.flamm.df.noNAs.predictors) %>%
          combinations(n = ncol(scaled.flamm.df.noNAs.predictors), r = i, repeats.allowed = F) %>% 
          apply(1, paste0, collapse = ' + ') # all possible combinations of models from 2 - # of predictors we're interested in (note: cannot include only 1 predictor, as this breaks multicollinearity for loop below; also, we're probably not interested in a model with only one explanatory variable)
  for(j in (1+length(call.vec)):(length(call)+length(call.vec))){ # adding linear mixed effects model to mod.list
      mod.list[[j]] <- lmer(as.formula(paste('fh', '~', call[j-length(call.vec)], '+', '(1|plant_id)', sep = '')), data = scaled.flamm.df.noNAs)
  }
  call.vec <- append(call.vec, call) # to index where to put model into model list
}
```

SECOND: FLAG ANY MODELS THAT BREAK MULTICOLINEARITY ASSUMPTIONS
```{r}
model.df <- data.frame(model.id = c(1:length(mod.list)),
                       call = c(rep(NA, length(mod.list))),
                       multicollinearity = c(rep(NA, length(mod.list))),
                       AIC = c(rep(NA, length(mod.list))),
                       BIC = c(rep(NA, length(mod.list))),
                       CP = c(rep(NA, length(mod.list))))

for(i in 1:length(mod.list)){
  vif.df <- multicollinearity(mod.list[[i]])
  ifelse(max(vif.df$VIF) > 5, model.df$multicollinearity[i] <- 'yes', 
             model.df$multicollinearity[i] <- 'no')
}

for(i in 1:length(mod.list)){
  if(model.df$multicollinearity[i] == 'yes'){
  mod.list[[i]] <- NA
}}
```

THIRD: CALCULATE AIC, BIC, MALLOWS CP
```{r}
for(i in 1:length(mod.list)) {
  if(model.df$multicollinearity[i] == 'yes')
  {model.df[i, 3:6] <- NA}
  else{
    model.df$call[i] <- paste(colnames(mod.list[[i]]@frame), collapse = ', ')
    model.df$AIC[i] <- AIC(mod.list[[i]])
    model.df$BIC[i] <- BIC(mod.list[[i]])
    model.df$CP[i] <- mallows.cp(mod.list[[i]], k = length(mod.list[[i]]@beta - 1))
  }
}
```

# Model Selection Function
This works ! , but we may alter as necessary
```{r}
scaled.flamm.df.noNAs.predictors <- scaled.flamm.df.noNAs %>% 
  select(lfm, mpa, ww_flam_sample, dw_flam_sample, sample_wt, branch_volume, leaf_mass_ratio, LMA, branching) # select all predictors that you're interested in looking at

# NOTE: y.var and rem.str must be a character vector, predictors must be a dataframe
mem.selection <- function(y.var, predictors, rem.str = '(1|plant_id)'){
# FIRST: List models
# empty list
mod.list <- list()
call.vec <- c()

# setting up a nested for loop to list all possible models including those predictors
for(i in 2:ncol(predictors)){
  call <- colnames(predictors) %>%
          combinations(n = ncol(predictors), r = i, repeats.allowed = F) %>% 
          apply(1, paste0, collapse = ' + ') # all possible combinations of models from 2 - # of predictors we're interested in (note: cannot include only 1 predictor, as this breaks multicollinearity for loop below; also, we're probably not interested in a model with only one explanatory variable)
  for(j in (1+length(call.vec)):(length(call)+length(call.vec))){ # adding linear mixed effects model to mod.list
      mod.list[[j]] <- lmer(as.formula(paste(y.var, '~', call[j-length(call.vec)], '+', rem.str, sep = '')), data = scaled.flamm.df.noNAs)
  }
  call.vec <- append(call.vec, call) # to index where to put model into model list
}

# CREATE DATAFRAME
model.df <- data.frame(model.id = c(1:length(mod.list)),
                       call = c(rep(NA, length(mod.list))),
                       multicollinearity = c(rep(NA, length(mod.list))),
                       AIC = c(rep(NA, length(mod.list))),
                       BIC = c(rep(NA, length(mod.list))),
                       CP = c(rep(NA, length(mod.list))))

# SECOND: FLAG ANY MODELS THAT BREAK MULTICOLINEARITY ASSUMPTIONS
for(i in 1:length(mod.list)){
  vif.df <- multicollinearity(mod.list[[i]])
  ifelse(max(vif.df$VIF) > 5, model.df$multicollinearity[i] <- 'yes', 
             model.df$multicollinearity[i] <- 'no')
}

for(i in 1:length(mod.list)){
  if(model.df$multicollinearity[i] == 'yes'){
  mod.list[[i]] <- NA
  }}

# THIRD: CALCULATE AIC, BIC, MALLOWS CP
for(i in 1:length(mod.list)) {
  if(model.df$multicollinearity[i] == 'yes')
  {model.df[i, 3:6] <- NA}
  else{
    model.df$call[i] <- paste(colnames(mod.list[[i]]@frame), collapse = ', ')
    model.df$AIC[i] <- AIC(mod.list[[i]])
    model.df$BIC[i] <- BIC(mod.list[[i]])
    model.df$CP[i] <- mallows.cp(mod.list[[i]], k = length(mod.list[[i]]@beta - 1))
  }
}
output <- list(model.df, mod.list)
return(output)
}
rm(mod.list, model.df, call.vec)
mod.list <- list()
call.vec <- c()
fh.mem.selection <- mem.selection('fh', scaled.flamm.df.noNAs.predictors)
fh.model.df <- fh.mem.selection[[1]]
fh.mod.list <- fh.mem.selection[[2]]
```

